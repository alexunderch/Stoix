# Taken and modified from https://github.com/instadeepai/sebulba
import abc
import logging
import queue
import threading
import traceback
from typing import Any, Callable, List, NamedTuple, Optional, Sequence, Tuple, Union

import chex
import jax
import jax.numpy as jnp

from stoix.base_types import (
    Action,
    Done,
    Extras,
    LossInfo,
    Observation,
    OptStates,
    Parameters,
    StoixState,
    Truncated,
)


class CoreLearnerState(NamedTuple):
    params: Parameters
    opt_states: OptStates


class BaseTrajectory(NamedTuple):
    done: Done
    truncated: Truncated
    action: Action
    reward: chex.Array
    extras: Extras
    obs: Observation
    next_obs: Observation


SebulbaLearnFn = Callable[
    [StoixState, BaseTrajectory, chex.PRNGKey],
    Tuple[StoixState, LossInfo],
]
SebulbaActorFn = Callable[
    [Parameters, Observation, chex.Array],
    Tuple[Action, Extras],
]
BaseTrajectoryProcessor = Callable[[BaseTrajectory], chex.ArrayTree]

Env = Any
EnvBuilder = Callable[[int], Env]


class StoppableComponent(threading.Thread, abc.ABC):
    """
    A `StoppableComponent` represents a component of the Sebulba system that is run on its own
    Thread that can be stopped by calling `stop`.

    You should subclass the `StoppableComponent` and override the `_run` method,
    making sure your component is gracefully stopped when `self.should_stop` is `True`.
    """

    def __init__(self, name: Union[str, None] = None):
        """
        Args:
            name: The name of the component. If `None`, the name of the class is used.
        """
        name = name if name is not None else self.__class__.__name__
        super().__init__(name=name, daemon=True)
        self.should_stop = False
        self.log = logging.getLogger(name)

    def stop(self) -> None:
        self.log.info("Asked to stop")
        self.should_stop = True

    def run(self) -> None:
        self.log.info("Starting")
        try:
            self._run()
        except Exception:
            self.log.error("Exception in thread:")
            traceback.print_exc()
        finally:
            self.log.info("End")

    @abc.abstractmethod
    def _run(self) -> None:
        pass


def shard_base_trajectory(traj: BaseTrajectory, devices: List[jax.Device]) -> BaseTrajectory:
    """
    Shard a `BaseTrajectory` across devices.
    """

    def shard(x: chex.ArrayTree, axis: int) -> chex.ArrayTree:
        x = jnp.split(x, len(devices), axis=axis)
        return jax.device_put_sharded(x, devices)

    # We create a mapping of the trajectory data to their corresponding shard axis.
    # We set it to one since we want to shard across the parallel env axis.
    traj_axis = BaseTrajectory(
        done=1,
        truncated=1,
        action=1,
        reward=1,
        extras=jax.tree_map(lambda x: 1, traj.extras),
        obs=jax.tree_map(lambda x: 1, traj.obs),
        next_obs=jax.tree_map(lambda x: 0, traj.next_obs),
    )
    return jax.tree_map(shard, traj, traj_axis)  # type: ignore


@jax.jit
def make_base_trajectory(
    traj_obs: Sequence[jax.Array],
    traj_term: Sequence[jax.Array],
    traj_trunc: Sequence[jax.Array],
    traj_actions: Sequence[jax.Array],
    traj_extras: Sequence[jax.Array],
    traj_rewards: Sequence[jax.Array],
    next_obs: Observation,
) -> BaseTrajectory:
    """
    Make a full trajectory from elements generated by an acting rollout and convert
    them to jax arrays.
    """
    return BaseTrajectory(
        done=jnp.asarray(traj_term),
        truncated=jnp.asarray(traj_trunc),
        action=jnp.asarray(traj_actions),
        reward=jnp.asarray(traj_rewards),
        extras=jax.tree_map(lambda *xs: jnp.array(xs), *traj_extras),
        obs=jax.tree_map(lambda *xs: jnp.array(xs), *traj_obs),
        next_obs=jax.tree_map(lambda xs: jnp.array(xs), next_obs),
    )


class Pipeline(StoppableComponent):
    """
    The `Pipeline` shards trajectories into `learner_devices`,
    ensuring trajectories are consumed in the right order to avoid being off-policy
    and limit the max number of samples in device memory at one time to avoid OOM issues.
    """

    def __init__(
        self,
        max_size: int,
        learner_devices: List[jax.Device],
        process_base_traj_fn: Optional[BaseTrajectoryProcessor] = None,
    ):
        """
        Initializes the pipeline with a maximum size and the devices to shard trajectories across.

        Args:
            max_size: The maximum number of trajectories to keep in the pipeline.
            learner_devices: The devices to shard trajectories across.
            process_base_traj_fn: A function to process the base trajectory before putting it in the pipeline.
                This can be used to put the base trajectory in the right format for the learner.
        """
        super().__init__("Pipeline")
        self.learner_devices = learner_devices
        self.tickets_queue: queue.Queue = queue.Queue()
        self._queue: queue.Queue = queue.Queue(maxsize=max_size)
        self.process_base_traj_fn = process_base_traj_fn
        if self.process_base_traj_fn is None:
            self.process_base_traj_fn = lambda x: x

    def _run(self) -> None:
        """
        This function ensures that trajectories on the queue are consumed in the right order. The
        start_condition and end_condition are used to ensure that only 1 thread is processing an
        item from the queue at one time, ensuring predictable memory usage.
        """
        while not self.should_stop:
            start_condition, end_condition = self.tickets_queue.get()
            with end_condition:
                with start_condition:
                    start_condition.notify()
                end_condition.wait()

    def put(
        self,
        traj_obs: Sequence[jax.Array],
        traj_term: Sequence[jax.Array],
        traj_trunc: Sequence[jax.Array],
        traj_actions: Sequence[jax.Array],
        traj_extras: Sequence[jax.Array],
        traj_rewards: Sequence[jax.Array],
        next_obs: Observation,
    ) -> None:
        """
        Put a trajectory on the queue to be consumed by the learner.
        """
        start_condition, end_condition = (threading.Condition(), threading.Condition())
        with start_condition:
            self.tickets_queue.put((start_condition, end_condition))
            start_condition.wait()  # wait to be allowed to start

        full_traj = make_base_trajectory(
            traj_obs, traj_term, traj_trunc, traj_actions, traj_extras, traj_rewards, next_obs
        )

        traj_sharded = shard_base_trajectory(full_traj, self.learner_devices)
        traj_sharded = self.process_base_traj_fn(traj_sharded)  # type: ignore
        self._queue.put(traj_sharded)

        with end_condition:
            end_condition.notify()  # tell we have finish

    def qsize(self) -> int:
        """Returns the number of trajectories in the pipeline."""
        return self._queue.qsize()

    def get(self, block: bool = True, timeout: Union[float, None] = None) -> Union[BaseTrajectory, chex.ArrayTree]:
        """Get a trajectory from the pipeline."""
        return self._queue.get(block, timeout)  # type: ignore


class ParamsSource(StoppableComponent):
    """
    A `ParamSource` is a component that allows networks params to be passed from a
    `Learner` component to `Actor` components.
    """

    def __init__(self, init_value: Parameters, device: jax.Device):
        super().__init__(f"ParamsSource-{device.id}")
        self.value = jax.device_put(init_value, device)
        self.device = device
        self.new_value: queue.Queue = queue.Queue()

    def _run(self) -> None:
        """
        This function is responsible for updating the value of the `ParamSource` when a new value
        is available.
        """
        while not self.should_stop:
            try:
                waiting = self.new_value.get(block=True, timeout=1)
                self.value = jax.device_put(jax.block_until_ready(waiting), self.device)
            except queue.Empty:
                continue

    def update(self, new_params: Parameters) -> None:
        """
        Update the value of the `ParamSource` with a new value.

        Args:
            new_params: The new value to update the `ParamSource` with.
        """
        self.new_value.put(new_params)

    def get(self) -> Parameters:
        """Get the current value of the `ParamSource`."""
        return self.value
